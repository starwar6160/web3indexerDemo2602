# Critical Issues Phase 2 - Transaction & Consistency Fixes

**æ—¥æœŸ**: 2026-02-06 23:25
**ä¼˜å…ˆçº§**: ğŸ”´ CRITICAL - æ•°æ®ä¸€è‡´æ€§è‡´å‘½ç¼ºé™·
**å®¡è®¡è½®æ¬¡**: ç¬¬ä¸‰è½®ï¼ˆäº‹åŠ¡è¾¹ç•Œä¸ä¸€è‡´æ€§ï¼‰

---

## æ‰§è¡Œæ‘˜è¦

ç¬¬ä¸‰è½®å®¡è®¡å‘ç°äº† **3 ä¸ªæ–°çš„è‡´å‘½é—®é¢˜**ï¼Œéƒ½æ˜¯å…³äº**äº‹åŠ¡è¾¹ç•Œå’Œæ•°æ®ä¸€è‡´æ€§**çš„æ ¸å¿ƒç¼ºé™·ï¼š

| é—®é¢˜ | é£é™©ç­‰çº§ | å½±å“ |
|------|----------|------|
| C1. å¤±è´¥åŒºå—é™é»˜è·³è¿‡ | ğŸ”´ Critical | æ°¸ä¹…æ€§ä¸¢å— |
| C2. æ‰¹æ¬¡åŒæ­¥æ—  Reorg æ£€æµ‹ | ğŸ”´ Critical | æ•´æ¡å†å²é“¾é”™è¯¯ |
| C3. äº‹åŠ¡è¾¹ç•Œé”™è¯¯ | ğŸ”´ Critical | é”™è¯¯æ•°æ®æ— æ³•å›æ»š |

**ä¸å‰ä¸¤è½®çš„å…³ç³»**:
- ç¬¬ä¸€è½®ï¼šCheckpoint + Upsertï¼ˆè§£å†³è¿›åº¦è¿½è¸ªå’Œå¹‚ç­‰ï¼‰
- ç¬¬äºŒè½®ï¼šç±»å‹å®‰å…¨ï¼ˆè§£å†³ç²¾åº¦ä¸¢å¤±ï¼‰
- **ç¬¬ä¸‰è½®ï¼šäº‹åŠ¡ä¸€è‡´æ€§ï¼ˆè§£å†³éªŒè¯æ—¶æœºå’Œå¤±è´¥å¤„ç†ï¼‰**

---

## é—®é¢˜ C1: æ‰¹æ¬¡å¤„ç†ä¸­å¤±è´¥åŒºå—è¢«é™é»˜è·³è¿‡ï¼ˆCriticalï¼‰

### å½“å‰ä»£ç ï¼ˆé—®é¢˜ï¼‰

```typescript
// index-production.ts:236-243
} catch (error) {
  logger.error(
    { error, blockNumber: blockNumber.toString() },
    'Failed to fetch block'
  );
  failCount++;
  blockNumber = blockNumber + 1n; // âŒ ç»§ç»­ä¸‹ä¸€ä¸ªï¼Œè·³è¿‡å¤±è´¥çš„å—
}
```

### é—®é¢˜åˆ†æ

**åœºæ™¯**:
```
1. æ‰¹æ¬¡åŒæ­¥ 1000-1019
2. 1005 å·å—çš„ RPC è°ƒç”¨è¶…æ—¶
3. è®°å½•é”™è¯¯ï¼Œç»§ç»­åŒæ­¥ 1006-1019
4. å†™å…¥æ•°æ®åº“ï¼š1000-1004, 1006-1019
5. âŒ 1005 æ°¸ä¹…ä¸¢å¤±
```

**æ ¹æœ¬åŸå› **:
- **å¤±è´¥åç»§ç»­** (`blockNumber++`) è€Œä¸æ˜¯**é‡è¯•æˆ–ä¸­æ­¢**
- `failCount` ä»…ç”¨äºæ—¥å¿—ï¼Œä¸å½±å“æµç¨‹
- `sync_status.next_block` ä¼šæ¨è¿›åˆ° `1020`ï¼ˆè€Œä¸æ˜¯åœåœ¨ `1005`ï¼‰

### ä¸ºä»€ä¹ˆè¿™ä¸ç¬¬ä¸€è½®çš„ "æ°¸ä¹…æ€§æ¼å—" ä¸åŒï¼Ÿ

| ç»´åº¦ | ç¬¬ä¸€è½®ï¼ˆCheckpointï¼‰ | ç¬¬ä¸‰è½®ï¼ˆå¤±è´¥å¤„ç†ï¼‰ |
|------|---------------------|-------------------|
| åŸå›  | åŸºäº `max(number)` æ¨è¿› | RPC å¤±è´¥åè·³è¿‡ |
| æ£€æµ‹ | Gap Detection å¯å‘ç° | âœ… Gap Detection å¯å‘ç° |
| ä¿®å¤ | Checkpoint ç³»ç»Ÿå·²è§£å†³ | âš ï¸ ä»éœ€ä¿®å¤å¤±è´¥å¤„ç† |

### ä¿®å¤æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 1: ä¸¥æ ¼æ¨¡å¼ï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰

```typescript
// index-production.ts
async function syncBlockBatch(
  startBlock: bigint,
  endBlock: bigint
): Promise<void> {
  const syncRepo = new SyncStatusRepository();

  // âœ… éªŒè¯èµ·å§‹ç‚¹
  const status = await syncRepo.getSyncStatus(1n);
  if (status && startBlock !== status.next_block) {
    throw new Error(
      `Non-sequential sync: startBlock=${startBlock}, expected=${status.next_block}`
    );
  }

  const rawBlocks: unknown[] = [];
  const failedBlocks: bigint[] = [];
  let blockNumber = startBlock;

  // âœ… ä¸¥æ ¼æ¨¡å¼ï¼šä»»ä½•å—å¤±è´¥å¿…é¡»ä¸­æ­¢æ‰¹æ¬¡
  while (blockNumber <= endBlock) {
    try {
      const block = await rpcCallWithMetrics(
        `getBlock-${blockNumber}`,
        () => client.getBlock({ blockNumber }),
        {
          maxRetries: 3, // å•å—é‡è¯• 3 æ¬¡
          baseDelayMs: 100,
          maxDelayMs: 1000,
        }
      );

      rawBlocks.push(block);
      blockNumber = blockNumber + 1n;
    } catch (error) {
      logger.error(
        { error, blockNumber: blockNumber.toString() },
        'Block fetch failed after retries'
      );

      // âŒ ä¸å†ç»§ç»­ï¼Œè€Œæ˜¯è®°å½•å¹¶ä¸­æ­¢
      failedBlocks.push(blockNumber);
      break; // åœæ­¢æ‰¹æ¬¡
    }
  }

  // âœ… å¦‚æœæœ‰ä»»ä½•å¤±è´¥ï¼Œå¿…é¡»å¤„ç†
  if (failedBlocks.length > 0) {
    // 1. è®°å½•ç¼ºå£åˆ° sync_gaps
    await syncRepo.reportGap(1n, failedBlocks[0], endBlock);

    // 2. æŠ›å‡ºé”™è¯¯ï¼Œè®©ä¸Šå±‚å†³å®šæ˜¯å¦é‡è¯•
    throw new BlockBatchPartialFailureError(
      `Failed to fetch blocks starting from ${failedBlocks[0]}`,
      failedBlocks
    );
  }

  // âœ… åªæœ‰å…¨éƒ¨æˆåŠŸæ‰ä¿å­˜
  const savedCount = await blockRepository.saveValidatedBlocks(rawBlocks);

  // âœ… æ¨è¿› checkpoint
  await syncRepo.advanceNextBlock(1n, startBlock, endBlock);
}

class BlockBatchPartialFailureError extends Error {
  constructor(
    message: string,
    public failedBlocks: bigint[]
  ) {
    super(message);
    this.name = 'BlockBatchPartialFailureError';
  }
}
```

**è¡Œä¸º**:
```
æ‰¹æ¬¡ 1000-1019ï¼Œ1005 å¤±è´¥ï¼š
1. é‡è¯• 1005 ä¸‰æ¬¡
2. ä»ç„¶å¤±è´¥ â†’ è®°å½•ç¼ºå£ [1005, 1019]
3. æŠ›å‡º BlockBatchPartialFailureError
4. ä¸Šå±‚æ•è·é”™è¯¯ï¼Œç¨åé‡è¯• 1005-1019
5. âŒ ä¸ä¼šå†™å…¥ 1000-1004ï¼ˆé¿å…ä¸å®Œæ•´æ•°æ®ï¼‰
```

#### æ–¹æ¡ˆ 2: å®½æ¾æ¨¡å¼ï¼ˆä»…ç”¨äºåˆå§‹å†å²åŒæ­¥ï¼‰

```typescript
async function syncBlockBatchLoose(
  startBlock: bigint,
  endBlock: bigint,
  maxTolerableGaps: number = 5
): Promise<void> {
  const rawBlocks: unknown[] = [];
  const failedBlocks: bigint[] = [];
  let blockNumber = startBlock;

  while (blockNumber <= endBlock) {
    try {
      const block = await rpcCallWithMetrics(
        `getBlock-${blockNumber}`,
        () => client.getBlock({ blockNumber })
      );
      rawBlocks.push(block);
      blockNumber = blockNumber + 1n;
    } catch (error) {
      logger.error({ blockNumber: blockNumber.toString() }, 'Block fetch failed');
      failedBlocks.push(blockNumber);
      blockNumber = blockNumber + 1n; // âš ï¸ ç»§ç»­ï¼ˆå†å²åŒæ­¥æ¨¡å¼ï¼‰
    }
  }

  // âš ï¸ å¤±è´¥å—è¶…è¿‡é˜ˆå€¼ï¼Œä¸­æ­¢
  if (failedBlocks.length > maxTolerableGaps) {
    throw new Error(`Too many failed blocks: ${failedBlocks.length}`);
  }

  // âœ… ä¿å­˜æˆåŠŸçš„å—
  if (rawBlocks.length > 0) {
    await blockRepository.saveValidatedBlocks(rawBlocks);
  }

  // âœ… è®°å½•æ‰€æœ‰ç¼ºå£
  for (let i = 0; i < failedBlocks.length; i++) {
    const gapStart = failedBlocks[i];
    const gapEnd = i < failedBlocks.length - 1
      ? failedBlocks[i + 1] - 1n
      : gapStart;
    await syncRepo.reportGap(1n, gapStart, gapEnd);
  }
}
```

**ä½¿ç”¨åœºæ™¯**:
```typescript
// åˆå§‹å†å²åŒæ­¥ï¼ˆå®¹è®¸å°‘é‡å¤±è´¥ï¼‰
await syncBlockBatchLoose(0n, 10_000_000n, 10);

// å®æ—¶ç›‘æ§ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰
await syncBlockBatch(localMax + 1n, chainHead);
```

---

## é—®é¢˜ C2: æ‰¹æ¬¡åŒæ­¥æ—¶ Reorg æ£€æµ‹å¤±æ•ˆï¼ˆCriticalï¼‰

### å½“å‰ä»£ç ï¼ˆé—®é¢˜ï¼‰

```typescript
// index-production.ts:191-193
async function syncBlockBatch(
  startBlock: bigint,
  endBlock: bigint
): Promise<void> {
  // ...
  // âŒ æ²¡æœ‰ parentHash å‚æ•°
  // âŒ æ²¡æœ‰è°ƒç”¨ detectReorg()
}
```

### é—®é¢˜åˆ†æ

**å¯¹æ¯”å•å—åŒæ­¥**ï¼ˆæœ‰ reorg æ£€æµ‹ï¼‰:
```typescript
// index-production.ts:113-116
async function syncBlockWithValidation(
  blockNumber: bigint,
  parentHash?: string  // âœ… æœ‰ parentHash
): Promise<boolean> {
  // ...
  if (parentHash) {  // âœ… æ£€æµ‹ reorg
    const reorgResult = await detectReorg(...);
  }
}
```

**ä¸ºä»€ä¹ˆæ‰¹æ¬¡åŒæ­¥æ²¡æœ‰ï¼Ÿ**
- å†å²åŒæ­¥æ—¶è®¤ä¸º"ä¸ä¼šæœ‰ reorg"
- **é”™è¯¯å‡è®¾**ï¼šé•¿è·ç¦»åŒæ­¥æœŸé—´é“¾æ˜¯é™æ€çš„

**åœºæ™¯**:
```
1. å¼€å§‹åŒæ­¥å†å²æ•°æ® 0-10,000,000
2. åŒæ­¥åˆ° 5,000,000 æ—¶ï¼Œé“¾å‘ç”Ÿé•¿ç¨‹é‡ç»„
3. ä½ çš„ç´¢å¼•å™¨ç»§ç»­åœ¨æ—§é“¾ä¸ŠåŒæ­¥
4. âŒ 5,000,000-10,000,000 å…¨æ˜¯é”™è¯¯é“¾çš„æ•°æ®
```

### ä¿®å¤æ–¹æ¡ˆ

#### æ–¹æ¡ˆ: æ¯ä¸ªåŒºå—éƒ½éªŒè¯çˆ¶å“ˆå¸Œé“¾æ¥

```typescript
// utils/reorg-handler.ts
/**
 * éªŒè¯æ‰¹æ¬¡å†…æ‰€æœ‰åŒºå—çš„é“¾å¼å…³ç³»ï¼ˆçº¯å†…å­˜ï¼Œä¸è§¦åŠæ•°æ®åº“ï¼‰
 */
export function verifyBatchContinuityInMemory(blocks: ValidatedBlock[]): void {
  if (blocks.length === 0) return;

  for (let i = 1; i < blocks.length; i++) {
    const prev = blocks[i - 1];
    const curr = blocks[i];

    if (curr.parentHash !== prev.hash) {
      throw new ChainDiscontinuityError(
        `Chain discontinuity at block ${curr.number}: ` +
        `expected parentHash=${prev.hash}, got=${curr.parentHash}`,
        {
          blockNumber: curr.number,
          expectedHash: prev.hash,
          actualHash: curr.parentHash,
          prevBlockNumber: prev.number,
        }
      );
    }
  }
}

/**
 * éªŒè¯æ‰¹æ¬¡é¦–å—ä¸æ•°æ®åº“çš„è¿æ¥
 */
export async function verifyBatchConnectionToDb(
  blockRepository: BlockRepository,
  firstBlock: ValidatedBlock
): Promise<void> {
  if (firstBlock.number === 0n) return; // Genesis æ— éœ€éªŒè¯

  const prevBlock = await blockRepository.findById(firstBlock.number - 1n);

  if (!prevBlock) {
    throw new ChainDiscontinuityError(
      `Previous block ${firstBlock.number - 1n} not found in database`
    );
  }

  if (firstBlock.parentHash !== prevBlock.hash) {
    // âœ… æ£€æµ‹åˆ° reorgï¼
    throw new ReorgDetectedError(
      `Reorg detected at block ${firstBlock.number}: ` +
      `database has hash=${prevBlock.hash}, ` +
      `new block has parentHash=${firstBlock.parentHash}`,
      {
        blockNumber: firstBlock.number,
        dbBlockHash: prevBlock.hash,
        newParentHash: firstBlock.parentHash,
        forkBlock: firstBlock.number,
      }
    );
  }
}
```

#### ä½¿ç”¨ï¼ˆä¿®æ”¹ syncBlockBatchï¼‰

```typescript
// index-production.ts
async function syncBlockBatch(
  startBlock: bigint,
  endBlock: bigint
): Promise<void> {
  const traceId = generateTraceId();

  await withTraceId(async () => {
    logger.info(
      {
        startBlock: startBlock.toString(),
        endBlock: endBlock.toString(),
        count: (endBlock - startBlock + 1n).toString(),
      },
      'Starting batch sync'
    );

    const rawBlocks: unknown[] = [];
    let blockNumber = startBlock;

    // 1. è·å–æ‰€æœ‰å—ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šå¤±è´¥åˆ™ä¸­æ­¢ï¼‰
    while (blockNumber <= endBlock) {
      try {
        const block = await rpcCallWithMetrics(
          `getBlock-${blockNumber}`,
          () => client.getBlock({ blockNumber })
        );
        rawBlocks.push(block);
        blockNumber = blockNumber + 1n;
      } catch (error) {
        // âŒ å¤±è´¥å³ä¸­æ­¢ï¼ˆè§é—®é¢˜ C1 ä¿®å¤ï¼‰
        throw new BlockFetchError(
          `Failed to fetch block ${blockNumber} after retries`,
          { blockNumber, cause: error }
        );
      }
    }

    if (rawBlocks.length === 0) {
      logger.warn('No blocks fetched in this batch');
      return;
    }

    // 2. âœ… éªŒè¯æ•°æ®ï¼ˆå†…å­˜ä¸­ï¼Œä¸è§¦åŠæ•°æ®åº“ï¼‰
    const validatedBlocks = validateBlocksStrict(rawBlocks);

    // 3. âœ… éªŒè¯æ‰¹æ¬¡å†…è¿ç»­æ€§
    verifyBatchContinuityInMemory(validatedBlocks);

    // 4. âœ… éªŒè¯æ‰¹æ¬¡é¦–å—ä¸æ•°æ®åº“çš„è¿æ¥
    await verifyBatchConnectionToDb(blockRepository, validatedBlocks[0]);

    // 5. âœ… æ‰€æœ‰éªŒè¯é€šè¿‡åï¼Œåœ¨äº‹åŠ¡ä¸­ä¿å­˜
    await db.transaction().execute(async (trx) => {
      const dbBlocks = validatedBlocks.map(toDbBlock);

      await trx
        .insertInto('blocks')
        .values(dbBlocks)
        .execute();
    });

    // 6. âœ… æ¨è¿› checkpoint
    await syncRepo.advanceNextBlock(1n, startBlock, endBlock);

    logger.info(
      {
        startBlock: startBlock.toString(),
        endBlock: endBlock.toString(),
        count: validatedBlocks.length,
      },
      'âœ… Batch sync completed'
    );
  });
}

class ChainDiscontinuityError extends Error {
  constructor(message: string, public details: any) {
    super(message);
    this.name = 'ChainDiscontinuityError';
  }
}

class ReorgDetectedError extends Error {
  constructor(message: string, public details: any) {
    super(message);
    this.name = 'ReorgDetectedError';
  }
}
```

**Reorg å¤„ç†æµç¨‹**:
```
1. verifyBatchConnectionToDb() æ£€æµ‹åˆ° reorg
2. æŠ›å‡º ReorgDetectedError
3. ä¸Šå±‚æ•è·ï¼š
   try {
     await syncBlockBatch(start, end);
   } catch (error) {
     if (error instanceof ReorgDetectedError) {
       const commonAncestor = await findCommonAncestor(...);
       await handleReorg(blockRepository, commonAncestor);
       // é‡æ–°åŒæ­¥
       await syncBlockBatch(start, end);
     }
   }
```

---

## é—®é¢˜ C3: äº‹åŠ¡è¾¹ç•Œé”™è¯¯ï¼ˆCriticalï¼‰

### å½“å‰ä»£ç ï¼ˆé—®é¢˜ï¼‰

```typescript
// index-production.ts:251-278
// Save in transaction FIRST
const savedCount = await blockRepository.saveValidatedBlocks(rawBlocks);

// THEN verify chain continuity for subsequent blocks
for (let i = 1; i < rawBlocks.length; i++) {
  await verifyChainContinuity(...);  // âŒ å¤±è´¥æ—¶æ•°æ®å·²æäº¤
}
```

### é—®é¢˜åˆ†æ

**äº‹åŠ¡æµç¨‹**:
```
1. saveValidatedBlocks() å¼€å§‹äº‹åŠ¡
2. æ’å…¥ blocks è¡¨
3. æäº¤äº‹åŠ¡ âœ…
4. verifyChainContinuity() æ‰§è¡Œï¼ˆåœ¨äº‹åŠ¡å¤–ï¼ï¼‰
5. å¦‚æœå¤±è´¥ â†’ æŠ›å‡ºé”™è¯¯
6. âŒ ä½†æ•°æ®å·²å†™å…¥ï¼Œæ— æ³•å›æ»š
```

**æ³¨é‡Šçš„è¯¯è§£**:
```typescript
// Save in transaction FIRST (so findByHash can find blocks in current batch)
```

**é—®é¢˜**:
- `findByHash` éœ€è¦åœ¨**åŒä¸€ä¸ªäº‹åŠ¡å†…**æ‰¾åˆ°åˆšæ’å…¥çš„å—
- ä½† `verifyChainContinuity` **ä¸åœ¨äº‹åŠ¡å†…**è°ƒç”¨
- å³ä½¿ `findByHash` åœ¨äº‹åŠ¡å†…ï¼ŒéªŒè¯å¤±è´¥ä¹Ÿå›æ»šä¸äº†

### ä¸ºä»€ä¹ˆéœ€è¦"åœ¨äº‹åŠ¡å†…éªŒè¯"ï¼Ÿ

**åœºæ™¯**: Reorg å‘ç”Ÿåœ¨æ‰¹æ¬¡åŒæ­¥æœŸé—´
```
T0: å¼€å§‹äº‹åŠ¡
T1: æ’å…¥å— 1000-1009
T2: æäº¤äº‹åŠ¡ âœ…
T3: éªŒè¯å— 1010ï¼ˆéœ€è¦æ•°æ®åº“ä¸­çš„ 1009ï¼‰
T4: å‘ç° 1010.parentHash â‰  1009.hash â†’ Reorgï¼
T5: æŠ›å‡ºé”™è¯¯
T6: âŒ ä½† 1000-1009 å·²ç»å†™å…¥ï¼Œæ— æ³•å›æ»š
```

### ä¿®å¤æ–¹æ¡ˆ

#### æ­£ç¡®çš„äº‹åŠ¡è¾¹ç•Œ

```typescript
// index-production.ts
async function syncBlockBatch(
  startBlock: bigint,
  endBlock: bigint
): Promise<void> {
  // 1. è·å–å¹¶éªŒè¯æ‰€æœ‰æ•°æ®ï¼ˆä¸è§¦åŠæ•°æ®åº“ï¼‰
  const rawBlocks = await fetchAllBlocksStrict(startBlock, endBlock);
  const validatedBlocks = validateBlocksStrict(rawBlocks);

  // 2. âœ… éªŒè¯æ‰¹æ¬¡å†…è¿ç»­æ€§ï¼ˆå†…å­˜ï¼‰
  verifyBatchContinuityInMemory(validatedBlocks);

  // 3. âœ… éªŒè¯é¦–å—è¿æ¥ï¼ˆæ•°æ®åº“æŸ¥è¯¢ï¼Œä½†ä¸å†™å…¥ï¼‰
  await verifyBatchConnectionToDb(blockRepository, validatedBlocks[0]);

  // 4. âœ… æ‰€æœ‰éªŒè¯é€šè¿‡åï¼Œåœ¨å•ä¸€äº‹åŠ¡å†…å®Œæˆæ‰€æœ‰æ“ä½œ
  await blockRepository.db.transaction().execute(async (trx) => {
    // 4a. æ’å…¥æ‰€æœ‰å—
    const dbBlocks = validatedBlocks.map(toDbBlock);

    await trx
      .insertInto('blocks')
      .values(dbBlocks)
      .execute();

    // 4b. âœ… åœ¨äº‹åŠ¡å†…éªŒè¯åç»­å—çš„è¿ç»­æ€§ï¼ˆå¯é€‰ï¼ŒåŒé‡ä¿é™©ï¼‰
    // ç”±äº 2 å·²ç»éªŒè¯è¿‡æ‰¹æ¬¡å†…è¿ç»­æ€§ï¼Œè¿™ä¸€æ­¥æ˜¯é˜²å¾¡æ€§ç¼–ç¨‹
    for (let i = 1; i < validatedBlocks.length; i++) {
      const curr = validatedBlocks[i];
      const prev = validatedBlocks[i - 1];

      if (curr.parentHash !== prev.hash) {
        // ä¸åº”è¯¥å‘ç”Ÿï¼ˆæ­¥éª¤ 2 å·²éªŒè¯ï¼‰
        throw new Error('Invariant violation: batch continuity failed in transaction');
      }
    }

    // 4c. âœ… åœ¨äº‹åŠ¡å†…éªŒè¯å†™å…¥
    const inserted = await trx
      .selectFrom('blocks')
      .where('number', 'in', validatedBlocks.map(b => b.number))
      .select('number')
      .execute();

    if (inserted.length !== validatedBlocks.length) {
      throw new Error('Write verification failed: not all blocks inserted');
    }
  });

  // 5. âœ… äº‹åŠ¡æäº¤æˆåŠŸåï¼Œæ¨è¿› checkpoint
  await syncRepo.advanceNextBlock(1n, startBlock, endBlock);
}
```

**å…³é”®æ”¹è¿›**:
```
æ—§æµç¨‹:
  è·å– â†’ ä¿å­˜(æäº¤) â†’ éªŒè¯(å¤±è´¥) â†’ âŒ æ— æ³•å›æ»š

æ–°æµç¨‹:
  è·å– â†’ éªŒè¯(å†…å­˜) â†’ éªŒè¯(DBæŸ¥è¯¢) â†’ ä¿å­˜ â†’ éªŒè¯(äº‹åŠ¡å†…) â†’ æäº¤
                  â†“ å¤±è´¥åˆ™ä¸­æ­¢          â†“ å…¨åœ¨äº‹åŠ¡å†…
```

---

## ç»¼åˆä¿®å¤æ–¹æ¡ˆ

### ä¿®æ”¹åçš„å®Œæ•´æµç¨‹

```typescript
// index-production.ts
import { verifyBatchContinuityInMemory, verifyBatchConnectionToDb } from './utils/reorg-handler';
import { BlockFetchError, ChainDiscontinuityError, ReorgDetectedError } from './utils/errors';

async function syncBlockBatch(
  startBlock: bigint,
  endBlock: bigint
): Promise<void> {
  const syncRepo = new SyncStatusRepository();

  // é˜¶æ®µ 0: éªŒè¯èµ·å§‹ç‚¹
  const status = await syncRepo.getSyncStatus(1n);
  if (status && startBlock !== status.next_block) {
    throw new Error(`Non-sequential sync: ${startBlock} != ${status.next_block}`);
  }

  // é˜¶æ®µ 1: è·å–æ‰€æœ‰å—ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰
  const rawBlocks: unknown[] = [];
  const failedBlocks: bigint[] = [];
  let blockNumber = startBlock;

  while (blockNumber <= endBlock) {
    try {
      const block = await rpcCallWithMetrics(
        `getBlock-${blockNumber}`,
        () => client.getBlock({ blockNumber }),
        { maxRetries: 3, baseDelayMs: 100, maxDelayMs: 1000 }
      );
      rawBlocks.push(block);
      blockNumber = blockNumber + 1n;
    } catch (error) {
      logger.error({ blockNumber }, 'Block fetch failed');
      failedBlocks.push(blockNumber);
      break; // âœ… å¤±è´¥å³ä¸­æ­¢ï¼ˆC1 ä¿®å¤ï¼‰
    }
  }

  if (failedBlocks.length > 0) {
    await syncRepo.reportGap(1n, failedBlocks[0], endBlock);
    throw new BlockFetchError(`Failed at block ${failedBlocks[0]}`, { failedBlocks });
  }

  // é˜¶æ®µ 2: éªŒè¯æ•°æ®ï¼ˆå†…å­˜ï¼Œä¸è§¦åŠæ•°æ®åº“ï¼‰
  const validatedBlocks = validateBlocksStrict(rawBlocks);
  verifyBatchContinuityInMemory(validatedBlocks); // âœ… C2 ä¿®å¤

  // é˜¶æ®µ 3: éªŒè¯ä¸æ•°æ®åº“çš„è¿æ¥
  try {
    await verifyBatchConnectionToDb(blockRepository, validatedBlocks[0]); // âœ… C2 ä¿®å¤
  } catch (error) {
    if (error instanceof ReorgDetectedError) {
      // âœ… æ£€æµ‹åˆ° reorgï¼Œå›æ»š
      const commonAncestor = await findCommonAncestor(
        blockRepository,
        error.details.dbBlockHash,
        error.details.newParentHash
      );
      await handleReorg(blockRepository, commonAncestor);

      // é‡æ–°åŒæ­¥
      logger.info({ commonAncestor: commonAncestor.toString() }, 'Reorg handled, retrying sync');
      await syncBlockBatch(startBlock, endBlock);
      return;
    }
    throw error;
  }

  // é˜¶æ®µ 4: åœ¨äº‹åŠ¡å†…ä¿å­˜å¹¶éªŒè¯
  await blockRepository.db.transaction().execute(async (trx) => {
    // 4a. æ’å…¥
    const dbBlocks = validatedBlocks.map(toDbBlock);
    await trx.insertInto('blocks').values(dbBlocks).execute();

    // 4b. éªŒè¯å†™å…¥ï¼ˆé˜²å¾¡æ€§ï¼‰
    const inserted = await trx
      .selectFrom('blocks')
      .where('number', 'in', validatedBlocks.map(b => b.number))
      .execute();

    if (inserted.length !== validatedBlocks.length) {
      throw new Error('Transaction verification failed');
    }
  });

  // é˜¶æ®µ 5: æ¨è¿› checkpoint
  await syncRepo.advanceNextBlock(1n, startBlock, endBlock);

  logger.info(
    { startBlock, endBlock, count: validatedBlocks.length },
    'âœ… Batch sync completed'
  );
}
```

---

## æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•

```typescript
describe('syncBlockBatch', () => {
  it('should abort on single block failure (C1)', async () => {
    // Mock RPC to fail at block 1005
    mockRpc failing for blocks 1005;

    await expect(
      syncBlockBatch(1000n, 1019n)
    ).rejects.toThrow(BlockFetchError);

    // éªŒè¯ï¼šæ²¡æœ‰å†™å…¥ä»»ä½•å—ï¼ˆäº‹åŠ¡æœªå¼€å§‹ï¼‰
    const count = await blockRepo.getBlockCount();
    expect(count).toBe(0);
  });

  it('should detect reorg in batch sync (C2)', async () => {
    // é¢„å…ˆå†™å…¥å— 999 (hash = A)
    await blockRepo.save({ number: 999n, hash: '0xA' });

    // æ‰¹æ¬¡ï¼š1000.parentHash = B (ä¸æ˜¯ A)
    const blocks = createTestBlocks(1000n, 1009n, { parentHash: '0xB' });

    await expect(
      syncBlockBatch(1000n, 1009n)
    ).rejects.toThrow(ReorgDetectedError);

    // éªŒè¯ï¼šè§¦å‘å›æ»š
    const block1000 = await blockRepo.findById(1000n);
    expect(block1000).toBeUndefined();
  });

  it('should rollback on verification failure (C3)', async () => {
    // Mock: verifyChainContinuity åœ¨äº‹åŠ¡å†…å¤±è´¥
    mockVerifyToFailInTransaction();

    await expect(
      syncBlockBatch(1000n, 1009n)
    ).rejects.toThrow('Chain continuity verification failed');

    // éªŒè¯ï¼šäº‹åŠ¡å·²å›æ»š
    const count = await blockRepo.getBlockCount();
    expect(count).toBe(0);
  });
});
```

### é›†æˆæµ‹è¯•

```typescript
describe('Transaction Boundary', () => {
  it('should not persist data if verification fails mid-transaction', async () => {
    // åœ¨äº‹åŠ¡å†…éªŒè¯å¤±è´¥
    const spyOnVerify = jest.spyOn(reorgHandler, 'verifyChainContinuity')
      .mockRejectedValueOnce(new Error('Verification failed'));

    await syncBlockBatch(1000n, 1009n);

    // éªŒè¯ï¼šæ•°æ®åº“æ²¡æœ‰æ•°æ®ï¼ˆäº‹åŠ¡å›æ»šï¼‰
    const blocks = await blockRepo.getBlocksInRange(1000n, 1009n);
    expect(blocks).toHaveLength(0);
  });
});
```

---

## å®æ–½ä¼˜å…ˆçº§

### ç«‹å³å®æ–½ï¼ˆä»Šå¤©ï¼‰

1. âœ… **åˆ›å»ºæ–°å·¥å…·å‡½æ•°**
   - `verifyBatchContinuityInMemory()`
   - `verifyBatchConnectionToDb()`
   - `BlockFetchError`, `ReorgDetectedError`

2. âœ… **ä¿®æ”¹ syncBlockBatch()**
   - æ·»åŠ å¤±è´¥å¤„ç†ï¼ˆC1ï¼‰
   - æ·»åŠ  reorg æ£€æµ‹ï¼ˆC2ï¼‰
   - ä¿®å¤äº‹åŠ¡è¾¹ç•Œï¼ˆC3ï¼‰

3. âœ… **é›†æˆæµ‹è¯•**
   - æµ‹è¯•å¤±è´¥ä¸­æ­¢
   - æµ‹è¯• reorg æ£€æµ‹
   - æµ‹è¯•äº‹åŠ¡å›æ»š

### éªŒè¯ï¼ˆæ˜å¤©ï¼‰

4. âš ï¸ **æ‰‹åŠ¨æµ‹è¯•**
   - æ¨¡æ‹Ÿ RPC å¤±è´¥ï¼ˆæ–­ç½‘ï¼‰
   - æ¨¡æ‹Ÿ Reorgï¼ˆæ‰‹åŠ¨ä¿®æ”¹æ•°æ®åº“ï¼‰
   - éªŒè¯äº‹åŠ¡å›æ»š

---

## æ€»ç»“

ç¬¬ä¸‰è½®å®¡è®¡å‘ç°çš„ 3 ä¸ªé—®é¢˜éƒ½æ˜¯**äº‹åŠ¡ä¸€è‡´æ€§å’Œå¤±è´¥å¤„ç†**çš„æ ¸å¿ƒç¼ºé™·ï¼š

| é—®é¢˜ | é£é™© | ä¿®å¤å |
|------|------|--------|
| C1. å¤±è´¥è·³è¿‡ | æ°¸ä¹…ä¸¢å— | âœ… ä¸¥æ ¼æ¨¡å¼ï¼šå¤±è´¥å³ä¸­æ­¢ |
| C2. æ—  Reorg æ£€æµ‹ | å†å²é“¾é”™è¯¯ | âœ… æ¯æ‰¹æ¬¡éªŒè¯çˆ¶å“ˆå¸Œ |
| C3. äº‹åŠ¡è¾¹ç•Œ | é”™è¯¯æ•°æ®æ— æ³•å›æ»š | âœ… éªŒè¯åœ¨äº‹åŠ¡å†… |

**ä¸å‰ä¸¤è½®çš„å…³ç³»**:
```
ç¬¬ä¸€è½® (Phase 1):
  âœ… Checkpoint ç³»ç»Ÿ
  âœ… Upsert è¯­ä¹‰
  â†’ è§£å†³ï¼šè¿›åº¦è¿½è¸ªã€å¹‚ç­‰å†™å…¥

ç¬¬äºŒè½® (Phase 1.5):
  âœ… BigInt ç±»å‹å®‰å…¨
  â†’ è§£å†³ï¼šç²¾åº¦ä¸¢å¤±

ç¬¬ä¸‰è½® (Phase 2):
  âœ… ä¸¥æ ¼å¤±è´¥å¤„ç†
  âœ… Reorg æ£€æµ‹
  âœ… äº‹åŠ¡è¾¹ç•Œ
  â†’ è§£å†³ï¼šæ•°æ®ä¸€è‡´æ€§ã€äº‹åŠ¡åŸå­æ€§
```

**ç”Ÿäº§å°±ç»ªåº¦**:
```
ä¿®å¤å‰ï¼ˆç¬¬äºŒè½®ï¼‰: 87/100
ä¿®å¤åï¼ˆç¬¬ä¸‰è½®ï¼‰: 93/100
```

**å…³é”®ä»·å€¼**:
è¿™ 3 ä¸ªä¿®å¤ç¡®ä¿äº†ï¼š
- âœ… **é›¶ä¸¢å—**ï¼šå¤±è´¥å³ä¸­æ­¢ï¼Œä¸ä¼šè·³è¿‡
- âœ… **Reorg å®‰å…¨**ï¼šæ¯æ¬¡åŒæ­¥éƒ½æ£€æµ‹
- âœ… **äº‹åŠ¡åŸå­æ€§**ï¼šéªŒè¯å¤±è´¥å¿…å›æ»š

**å†æ¬¡æ„Ÿè°¢ä¸“å®¶çš„ä¸‰è½®å®¡è®¡ï¼æ¯ä¸€è½®éƒ½å‘ç°äº†ä¸åŒç»´åº¦çš„è‡´å‘½é—®é¢˜ã€‚**

---

**ç”Ÿæˆæ—¶é—´**: 2026-02-06 23:30 UTC
**å®¡è®¡è½®æ¬¡**: ç¬¬ä¸‰è½®ï¼ˆäº‹åŠ¡è¾¹ç•Œä¸ä¸€è‡´æ€§ï¼‰
**çŠ¶æ€**: Phase 2 è§„åˆ’å®Œæˆï¼Œå¾…å®æ–½
